{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_file():\n",
    "    df = pd.read_parquet(\"logos.snappy.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "    total_domains = len(df[\"domain\"])\n",
    "    unique_domains = set(df[\"domain\"])\n",
    "    num_duplicates = total_domains - len(unique_domains)\n",
    "\n",
    "    with open(\"logos.txt\", \"w\") as file:\n",
    "        for domain in unique_domains:\n",
    "            file.write(domain + \"\\n\")\n",
    "\n",
    "    print(f\"Total domains: {total_domains}\")\n",
    "    print(f\"Unique domains: {len(unique_domains)}\")\n",
    "    print(f\"Duplicates found: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import queue\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "success = 0\n",
    "lock = threading.Lock()\n",
    "error_lock = threading.Lock()\n",
    "\n",
    "class LogoDownloaderWorker(threading.Thread):\n",
    "    def __init__(self, task_queue):\n",
    "        super().__init__()\n",
    "        self.task_queue = task_queue\n",
    "    \n",
    "    def download_logo(self, domain, output_path):\n",
    "        url = f\"https://logo.clearbit.com/{domain}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                with open(output_path, \"wb\") as file:\n",
    "                    file.write(response.content)\n",
    "                with lock:\n",
    "                    global success\n",
    "                    success += 1\n",
    "                return True\n",
    "            \n",
    "            else:\n",
    "                with error_lock:\n",
    "                    with open(\"log.txt\", \"a\") as file:\n",
    "                        file.write(f\"{domain}\\n\")\n",
    "                return False\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            with error_lock:\n",
    "                with open(\"log.txt\", \"a\") as file:\n",
    "                    file.write(f\"{domain}\\n\")\n",
    "            return False\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                domain = self.task_queue.get(timeout=1)\n",
    "            except queue.Empty:\n",
    "                break\n",
    "            \n",
    "            output_path = os.path.join(\"logos\", f\"{domain.replace('.', '_')}.png\")\n",
    "            self.download_logo(domain, output_path)\n",
    "            self.task_queue.task_done()\n",
    "\n",
    "        \n",
    "def master_thread(num_workers = 20):\n",
    "    os.makedirs(\"logos\", exist_ok=True)\n",
    "\n",
    "    with open(\"logos.txt\") as file:\n",
    "        domains = file.read().splitlines()\n",
    "    \n",
    "    task_queue = queue.Queue()\n",
    "    for domain in domains:\n",
    "        task_queue.put(domain)\n",
    "    \n",
    "    workers = [LogoDownloaderWorker(task_queue) for _ in range(num_workers)]\n",
    "    for worker in workers:\n",
    "        worker.start()\n",
    "    \n",
    "    with tqdm(total=len(domains), desc = \"Downloading logos: \") as progress_bar:\n",
    "        path = \"logos\"\n",
    "        while not task_queue.empty():\n",
    "            num_files = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "            progress_bar.update(num_files - progress_bar.n)\n",
    "            time.sleep(1)\n",
    "\n",
    "    task_queue.join()\n",
    "\n",
    "    for worker in workers:\n",
    "        worker.join()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import threading\n",
    "import queue\n",
    "from bs4 import BeautifulSoup\n",
    "import cairosvg\n",
    "\n",
    "class HTMLLogoScraperWorker(threading.Thread):\n",
    "    def __init__(self, task_queue):\n",
    "        super().__init__()\n",
    "        self.task_queue = task_queue\n",
    "    \n",
    "    def find_and_download_logo(self, domain):\n",
    "        try:\n",
    "            response = requests.get(f\"https://{domain}\", timeout=5)\n",
    "            if response.status_code != 200:\n",
    "                return\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            if domain == \"deswam.in\":\n",
    "                with open(\"deswam.in.html\", \"w\") as file:\n",
    "                    file.write(response.text)\n",
    "            \n",
    "            img_tags = soup.find_all(\"img\")\n",
    "            for img in img_tags:\n",
    "                src = img.get(\"src\", \"\")\n",
    "                alt = img.get(\"alt\", \"\").lower()\n",
    "                verifClass = \"\".join(img.get(\"class\", \"\")).lower()\n",
    "\n",
    "                if \"logo\" in src.lower() or \"logo\" in alt or \"logo\" in verifClass:\n",
    "                    logo_url = src if src.startswith(\"http\") else f\"https://{domain}{src}\"\n",
    "                    self.download_logo(logo_url, domain)\n",
    "                    return\n",
    "\n",
    "        except requests.RequestException:\n",
    "            print(f\"[ERROR] No logo in html source {domain}\")\n",
    "            pass\n",
    "\n",
    "    def download_logo(self, url, domain):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                file_extension = url.split(\".\")[-1].lower()\n",
    "                content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "\n",
    "                output_path = os.path.join(\"logos\", f\"{domain.replace('.', '_')}\")\n",
    "                \n",
    "                if file_extension == \"svg\" or \"image/svg+xml\" in content_type:\n",
    "                    svg_data = response.content.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "                    svg_path = f\"{output_path}.svg\"\n",
    "                    png_path = f\"{output_path}.png\"\n",
    "\n",
    "                    with open(svg_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                        file.write(svg_data)\n",
    "\n",
    "                    cairosvg.svg2png(url=svg_path, write_to=png_path)\n",
    "                    os.remove(svg_path)\n",
    "\n",
    "                else:\n",
    "                    with open(f\"{output_path}.png\", \"wb\") as file:\n",
    "                        file.write(response.content)\n",
    "\n",
    "        except requests.RequestException:\n",
    "            print(f\"[ERROR] Eșec la descărcarea logo-ului pentru {domain}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Eroare la procesarea SVG pentru {domain}: {e}\")\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                domain = self.task_queue.get(timeout=1)\n",
    "            except queue.Empty:\n",
    "                break\n",
    "            \n",
    "            self.find_and_download_logo(domain)\n",
    "            self.task_queue.task_done()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def scrape_html_logos(num_workers=10):\n",
    "    if not os.path.exists(\"log.txt\"):\n",
    "        print(\"No failed domains to process.\")\n",
    "        return\n",
    "\n",
    "    with open(\"log.txt\") as file:\n",
    "        failed_domains = list(set(file.read().splitlines()))\n",
    "    \n",
    "    if not failed_domains:\n",
    "        print(\"No failed domains to process.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Retrying {len(failed_domains)} domains by parsing HTML...\")\n",
    "\n",
    "    task_queue = queue.Queue()\n",
    "    for domain in failed_domains:\n",
    "        task_queue.put(domain)\n",
    "\n",
    "    workers = [HTMLLogoScraperWorker(task_queue) for _ in range(num_workers)]\n",
    "    for worker in workers:\n",
    "        worker.start()\n",
    "\n",
    "    task_queue.join()\n",
    "\n",
    "    for worker in workers:\n",
    "        worker.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file()\n",
    "master_thread(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_html_logos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Warning: File not found - {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Warning: Unable to read image - {image_path}\")\n",
    "        return None\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    return model(img).detach().cpu().numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"logos\"\n",
    "image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
    "\n",
    "feature_list = [extract_features(path) for path in image_paths]\n",
    "feature_list = [feat for feat in feature_list if feat is not None]\n",
    "feature_array = np.array(feature_list, dtype=np.float32)\n",
    "\n",
    "d = feature_array.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(feature_array)\n",
    "\n",
    "num_clusters = 150\n",
    "clustering = faiss.Clustering(d, num_clusters)\n",
    "clustering.train(feature_array, index)\n",
    "\n",
    "_, labels = index.search(feature_array, 1)\n",
    "\n",
    "labels = labels.flatten()\n",
    "\n",
    "clusters = {i: [] for i in range(num_clusters)}\n",
    "for i, label in enumerate(labels):\n",
    "    clusters[label].append(image_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster(cluster_id, num_images=10):\n",
    "    images = clusters[cluster_id][:num_images]\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, img_path in enumerate(images):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cluster_images_to_directory(output_dir, num_images=10):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for cluster_id, images in clusters.items():\n",
    "        cluster_dir = os.path.join(output_dir, f\"cluster_{cluster_id}\")\n",
    "        os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Salvăm imagini pentru Cluster {cluster_id} - {len(images)} imagini\")\n",
    "\n",
    "        for i, img_path in enumerate(images[:num_images]):\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"[Eroare] Imaginea nu a fost găsită: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"[Eroare] OpenCV nu a putut încărca imaginea: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            output_image_path = os.path.join(cluster_dir, f\"{cluster_id}_image_{i + 1}.png\")\n",
    "\n",
    "            cv2.imwrite(output_image_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"Imaginile au fost salvate în directorul {output_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"output_clusters\"\n",
    "save_cluster_images_to_directory(output_directory, num_images=80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
